
"""

æ¨¡ä»¿ï¼š
https://github.com/PromtEngineer/LangExtract-RAG


LangExtract + RAG æ¼”ç¤ºï¼ˆä¸­æ–‡å¤§ä¼—ç‚¹è¯„è¯„è®ºåœºæ™¯ï¼‰
ç»“æ„ä¸åŸç¤ºä¾‹ä¿æŒä¸€è‡´ï¼š
- Documentsï¼šç”± get_sample_documents() è¿”å›é™æ€æ ·æœ¬ï¼ˆæœª chunkï¼‰
- LangExtractï¼šå°è¯•ä½¿ç”¨ langextractï¼Œå¤±è´¥å›é€€åˆ°æ­£åˆ™æŠ½å–ï¼›æŠ½å–åè§„èŒƒåŒ– metadata
- Vector DBï¼šæœªçœŸæ­£å‘é‡åŒ–ï¼Œä»…ç”¨ SmartVectorStore åœ¨å†…å­˜åˆ—è¡¨ä¸­æ¨¡æ‹Ÿç´¢å¼•
- æ£€ç´¢ï¼šåŸºäºå…ƒæ•°æ®æ¨¡ç³ŠåŒ¹é…ä¸å­ä¸²åŒ¹é…
- Query -> Filter -> è¿”å›åŒ¹é…æ–‡æ¡£
"""


import os
import re
from typing import List, Dict
from dotenv import load_dotenv

# aliyun
from langextract.providers.openai import OpenAILanguageModel

apikey = 'sk-81f45edbfae243c3bfd2d8ff82f034ac'
model= OpenAILanguageModel(
            model_id='qwen-plus',
            base_url='https://dashscope.aliyuncs.com/compatible-mode/v1',
            api_key=apikey
            )



load_dotenv()

def get_sample_documents():
    """è¿”å›è‹¥å¹²ä¸­æ–‡å¤§ä¼—ç‚¹è¯„é£æ ¼çš„è¯„è®ºæ ·æœ¬ï¼ˆé™æ€ç¤ºä¾‹ï¼‰"""
    return [
        {
            "id": "rev_001",
            "title": "è€ç‹çƒ§çƒ¤ - ä¸€æ¬¡æ„‰å¿«çš„å°±é¤ä½“éªŒ",
            "content": "åº—åï¼šè€ç‹çƒ§çƒ¤\nè¯„åˆ†ï¼š5æ˜Ÿ\næ—¶é—´ï¼š2024-08-12\nè¯„ä»·ï¼šå‘³é“å¾ˆæ£’ï¼Œç¾Šè‚‰ä¸²å¤šæ±ï¼Œç¯å¢ƒå¹²å‡€ï¼ŒæœåŠ¡æ€åº¦çƒ­æƒ…ã€‚ä»·æ ¼é€‚ä¸­ï¼Œæ¨èç»™æœ‹å‹ã€‚æ ‡ç­¾ï¼šç¯å¢ƒå¥½, æœåŠ¡å¥½, å‘³é“æ£’"
        },
        {
            "id": "rev_002",
            "title": "å°å—é¢é¦† - é¢æ¡ä¸€èˆ¬ï¼ŒæœåŠ¡è¾ƒæ…¢",
            "content": "åº—åï¼šå°å—é¢é¦†\nè¯„åˆ†ï¼š3æ˜Ÿ\næ—¶é—´ï¼š2023-11-02\nè¯„ä»·ï¼šæ±¤é¢å‘³é“ä¸­è§„ä¸­çŸ©ï¼Œåˆ†é‡åå°‘ã€‚ä¸Šèœæ…¢ï¼ŒæœåŠ¡å‘˜ä¸å¤ªçƒ­æƒ…ã€‚äººå‡åè´µã€‚æ ‡ç­¾ï¼šå‘³é“ä¸€èˆ¬, ä¸Šèœæ…¢, åè´µ"
        },
        {
            "id": "rev_003",
            "title": "ç»¿èŒ¶é¤å… - å•†åŠ¡èšé¤é¦–é€‰",
            "content": "åº—åï¼šç»¿èŒ¶é¤å…\nè¯„åˆ†ï¼š4æ˜Ÿ\næ—¶é—´ï¼š2024-04-05\nè¯„ä»·ï¼šç¯å¢ƒé›…è‡´ï¼Œé€‚åˆèšé¤ã€‚èœå“å£å‘³ç¨³å®šï¼Œä»·æ ¼ç•¥é«˜ä½†æœåŠ¡åˆ°ä½ã€‚åœè½¦æ–¹ä¾¿ã€‚æ ‡ç­¾ï¼šç¯å¢ƒå¥½, é€‚åˆèšé¤, æœåŠ¡å¥½"
        },
        {
            "id": "rev_004",
            "title": "æµ·é²œä¸€å“ - é£Ÿææ–°é²œä½†ä»·æ ¼é«˜",
            "content": "åº—åï¼šæµ·é²œä¸€å“\nè¯„åˆ†ï¼š2æ˜Ÿ\næ—¶é—´ï¼š2024-06-20\nè¯„ä»·ï¼šæµ·é²œç¡®å®æ–°é²œï¼Œä½†ä»½é‡å°‘ä¸”ä»·æ ¼æ˜æ˜¾åé«˜ã€‚æœåŠ¡æ€åº¦ä¸€èˆ¬ï¼Œæœ‰ç‚¹å¤±æœ›ã€‚æ ‡ç­¾ï¼šé£Ÿææ–°é²œ, åè´µ, æœåŠ¡ä¸€èˆ¬"
        }
    ]


class FixedLangExtractProcessor:
    """é¢å‘ä¸­æ–‡ç‚¹è¯„çš„å…ƒæ•°æ®æŠ½å–å™¨ï¼›ä¼˜å…ˆä½¿ç”¨ langextractï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨æ­£åˆ™å›é€€"""

    def __init__(self):
        try:
            import langextract as lx
            self.lx = lx
            self.setup_complete = True
            print("âœ… LangExtract å·²åˆå§‹åŒ–ï¼ˆä¸­æ–‡æ¨¡å¼ï¼‰")
        except ImportError:
            print("âš ï¸  æœªå®‰è£… langextractï¼Œä½¿ç”¨æ­£åˆ™å›é€€é€»è¾‘")
            self.setup_complete = False

    def extract_metadata(self, documents: List[Dict]) -> List[Dict]:
        """å¯¹å¤šä¸ªæ–‡æ¡£æŠ½å–å¹¶è§„èŒƒåŒ– metadata"""
        if not self.setup_complete:
            return self._enhanced_regex_extraction(documents)

        # è¿™é‡Œç»™ langextract çš„ promptï¼ˆä¸­æ–‡æè¿°ï¼‰
        prompt = """
        ä»ä¸­æ–‡é¤å…è¯„è®ºä¸­æå–ä»¥ä¸‹å­—æ®µï¼š
        1. shop_name: é¤å…åç§°
        2. rating: è¯„åˆ†ï¼Œä»…è¿”å›æ•°å­—ï¼Œæ¯”å¦‚ "5" æˆ– "3"
        3. review_date: è¯„è®ºæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DDï¼ˆå¦‚èƒ½æŠ½å–ï¼‰
        4. review_focus: è¯„è®ºå…³æ³¨ç‚¹ï¼Œå¿…é¡»ä»ï¼š'å£å‘³', 'ç¯å¢ƒ', 'æœåŠ¡', 'ä»·æ ¼' ä¸­é€‰æ‹©æœ€ä¸»è¦çš„ä¸€é¡¹
        5. tags: è¯„è®ºä¸­çš„æ ‡ç­¾åˆ—è¡¨ï¼ˆå¦‚ ç¯å¢ƒå¥½ï¼ŒæœåŠ¡å¥½ï¼‰
        6. sentiment: æ•´ä½“æƒ…æ„Ÿï¼Œ'positive'/'negative'/'neutral'
        è¯·å°½é‡ç²¾ç¡®ï¼Œåªè¾“å‡ºå­—æ®µå€¼ï¼Œä¸è¦é¢å¤–è§£é‡Šã€‚
        """

        examples = [
            self.lx.data.ExampleData(
                text="åº—åï¼šç¤ºä¾‹åº—\nè¯„åˆ†ï¼š5æ˜Ÿ\næ—¶é—´ï¼š2024-01-01\nè¯„ä»·ï¼šå‘³é“å¾ˆå¥½ï¼ŒæœåŠ¡çƒ­æƒ…ã€‚æ ‡ç­¾ï¼šå‘³é“å¥½, æœåŠ¡å¥½",
                extractions=[
                    self.lx.data.Extraction(extraction_class="shop_name", extraction_text="ç¤ºä¾‹åº—", attributes={}),
                    self.lx.data.Extraction(extraction_class="rating", extraction_text="5", attributes={}),
                    self.lx.data.Extraction(extraction_class="review_date", extraction_text="2024-01-01", attributes={}),
                    self.lx.data.Extraction(extraction_class="review_focus", extraction_text="å£å‘³", attributes={}),
                    self.lx.data.Extraction(extraction_class="tags", extraction_text="å‘³é“å¥½, æœåŠ¡å¥½", attributes={}),
                    self.lx.data.Extraction(extraction_class="sentiment", extraction_text="positive", attributes={}),
                ]
            )
        ]

        extracted_docs = []
        for doc in documents:
            print(f"ğŸ“„ å¤„ç†æ–‡æ¡£: {doc['title']}")
            try:
                result = self.lx.extract(
                    text_or_documents=doc['content'],
                    prompt_description=prompt,
                    examples=examples,
                    model= model,
                    extraction_passes=2
                )
                metadata = self._process_and_normalize(result.extractions, doc)
            except Exception as e:
                print(f"  âš ï¸ LangExtract æŠ½å–å¤±è´¥: {e}")
                metadata = self._enhanced_regex_extraction([doc])[0]['metadata']

            extracted_docs.append({
                'id': doc['id'],
                'title': doc['title'],
                'content': doc['content'],
                'metadata': metadata
            })
        return extracted_docs

    def _process_and_normalize(self, extractions, doc: Dict) -> Dict:
        """å¤„ç† langextract çš„æŠ½å–ç»“æœå¹¶åšè§„èŒƒåŒ–"""
        metadata = {
            'shop': 'æœªçŸ¥',
            'rating': 'unknown',
            'date': '',
            'focus': 'å£å‘³',
            'tags': [],
            'sentiment': 'neutral'
        }

        for extraction in extractions:
            cls = extraction.extraction_class
            txt = extraction.extraction_text.strip() if hasattr(extraction, 'extraction_text') else ''
            if cls == "shop_name":
                metadata['shop'] = txt
            elif cls == "rating":
                # åªä¿ç•™æ•°å­—éƒ¨åˆ†
                m = re.search(r'(\d)', txt)
                if m:
                    metadata['rating'] = m.group(1)
                else:
                    metadata['rating'] = txt
            elif cls == "review_date":
                metadata['date'] = txt
            elif cls == "review_focus":
                metadata['focus'] = txt
            elif cls == "tags":
                # å¯èƒ½æ˜¯é€—å·åˆ†éš”
                tags = [t.strip() for t in re.split(r'[ï¼Œ,ï¼›;]', txt) if t.strip()]
                metadata['tags'] = tags
            elif cls == "sentiment":
                metadata['sentiment'] = txt

        # å›é€€ï¼šè‹¥å…³é”®å­—æ®µç¼ºå¤±ï¼Œä½¿ç”¨æ­£åˆ™å›é€€
        if metadata['shop'] == 'æœªçŸ¥' or metadata['rating'] == 'unknown':
            regex_meta = self._enhanced_regex_extraction([doc])[0]['metadata']
            if metadata['shop'] == 'æœªçŸ¥':
                metadata['shop'] = regex_meta['shop']
            if metadata['rating'] == 'unknown':
                metadata['rating'] = regex_meta['rating']
            if not metadata['tags']:
                metadata['tags'] = regex_meta['tags']

        return metadata

    def _enhanced_regex_extraction(self, documents: List[Dict]) -> List[Dict]:
        """é’ˆå¯¹ä¸­æ–‡ç‚¹è¯„çš„æ­£åˆ™æŠ½å–é€»è¾‘"""
        extracted_docs = []
        for doc in documents:
            metadata = {
                'shop': 'æœªçŸ¥',
                'rating': 'unknown',
                'date': '',
                'focus': 'å£å‘³',
                'tags': [],
                'sentiment': 'neutral'
            }
            title = doc.get('title', '')
            content = doc.get('content', '')

            # åº—åä¼˜å…ˆä» content ä¸­çš„ "åº—åï¼šXXX" æå–ï¼Œå…¶æ¬¡å°è¯•ä» title æå– "åº—å - æè¿°"
            shop_match = re.search(r'åº—å[:ï¼š]\s*([^\n]+)', content)
            if shop_match:
                metadata['shop'] = shop_match.group(1).strip()
            else:
                # title ä¸­å¯èƒ½æ˜¯ "åº—å - æè¿°"
                title_match = re.match(r'([\u4e00-\u9fff\w\s]+)\s*[-â€“â€”]\s*', title)
                if title_match:
                    metadata['shop'] = title_match.group(1).strip()

            # è¯„åˆ†ï¼šå¯»æ‰¾ "5æ˜Ÿ", "è¯„åˆ†ï¼š5" æˆ–å•ç‹¬æ•°å­—ï¼ˆ1-5ï¼‰
            rating_match = re.search(r'(\d)\s*æ˜Ÿ|è¯„åˆ†[:ï¼š]\s*(\d)', content)
            if rating_match:
                metadata['rating'] = rating_match.group(1) if rating_match.group(1) else rating_match.group(2)
            else:
                # å°è¯•åœ¨ title ä¸­æ‰¾
                r2 = re.search(r'(\d)\s*æ˜Ÿ', title)
                if r2:
                    metadata['rating'] = r2.group(1)

            # æ—¥æœŸï¼šç®€å•åŒ¹é… YYYY-MM-DD
            date_match = re.search(r'(\d{4}-\d{1,2}-\d{1,2})', content)
            if date_match:
                metadata['date'] = date_match.group(1)

            # tagsï¼šå¯»æ‰¾ "æ ‡ç­¾ï¼š" æˆ–å¥ä¸­å¸¸è§çŸ­è¯­
            tags_match = re.search(r'æ ‡ç­¾[:ï¼š]\s*([^\n]+)', content)
            if tags_match:
                tags = [t.strip() for t in re.split(r'[ï¼Œ,ï¼›;]', tags_match.group(1)) if t.strip()]
                metadata['tags'] = tags
            else:
                # ç®€å•å…³é”®è¯è¯†åˆ«ä½œä¸º tags
                possible_tags = []
                for kw in ['ç¯å¢ƒå¥½', 'æœåŠ¡å¥½', 'å‘³é“æ£’', 'å‘³é“ä¸€èˆ¬', 'åè´µ', 'é£Ÿææ–°é²œ', 'ä¸Šèœæ…¢', 'é€‚åˆèšé¤']:
                    if kw in content:
                        possible_tags.append(kw)
                metadata['tags'] = possible_tags

            # å…³æ³¨ç‚¹ï¼ˆfocusï¼‰ï¼šæ ¹æ®å…³é”®è¯åˆ¤æ–­
            if any(k in content for k in ['å‘³', 'å£å‘³', 'å¥½åƒ', 'éš¾åƒ']):
                metadata['focus'] = 'å£å‘³'
            elif any(k in content for k in ['ç¯å¢ƒ', 'é›…è‡´', 'å¹²å‡€', 'å˜ˆæ‚']):
                metadata['focus'] = 'ç¯å¢ƒ'
            elif any(k in content for k in ['æœåŠ¡', 'ä¸Šèœ', 'æ€åº¦']):
                metadata['focus'] = 'æœåŠ¡'
            elif any(k in content for k in ['ä»·æ ¼', 'åè´µ', 'ä¾¿å®œ', 'äººå‡']):
                metadata['focus'] = 'ä»·æ ¼'

            # æƒ…æ„Ÿåˆ¤å®šï¼ˆç²—ç•¥ï¼‰
            if any(p in content for p in ['å¾ˆå¥½', 'æ£’', 'æ¨è', 'æ»¡æ„', 'æ„‰å¿«', 'å–œæ¬¢']):
                metadata['sentiment'] = 'positive'
            elif any(n in content for n in ['å·®', 'å¤±æœ›', 'ä¸æ»¡', 'ä¸å¥½', 'å¤ªè´µ', 'ä¸€èˆ¬']):
                metadata['sentiment'] = 'negative'
            else:
                metadata['sentiment'] = 'neutral'

            extracted_docs.append({
                'id': doc['id'],
                'title': doc['title'],
                'content': doc['content'],
                'metadata': metadata
            })
        return extracted_docs


class SmartVectorStore:
    """å†…å­˜çº§åˆ«çš„â€œæ™ºèƒ½â€ç´¢å¼•ï¼šåŸºäºå…ƒæ•°æ®çš„æ¨¡ç³ŠåŒ¹é… + æ–‡æœ¬å­ä¸²åŒ¹é…ï¼ˆæœªåšå‘é‡åŒ–ï¼‰"""

    def __init__(self):
        self.documents = []

    def add_documents(self, docs: List[Dict]):
        self.documents = docs
        print(f"âœ… å·²ç´¢å¼• {len(docs)} æ¡è¯„è®º")

    def search(self, query: str, filters: Dict = None) -> List[Dict]:
        """åŸºäº query çš„ç®€å•æ£€ç´¢ï¼›è‹¥æä¾› filters åˆ™åšå…ƒæ•°æ®è¿‡æ»¤"""
        if not filters:
            return [doc for doc in self.documents if any(word.lower() in doc['content'].lower() for word in query.split())]

        filtered_docs = []
        for doc in self.documents:
            match = True
            md = doc.get('metadata', {})

            # åº—é“ºæ¨¡ç³ŠåŒ¹é…ï¼ˆæ”¯æŒéƒ¨åˆ†å…³é”®è¯åŒ¹é…ï¼‰
            if 'shop' in filters:
                q_shop = filters['shop'].lower()
                doc_shop = md.get('shop', 'æœªçŸ¥').lower()
                if q_shop not in doc_shop and doc_shop not in q_shop:
                    q_keywords = set(re.sub(r'(åº—|é¤å…|é¦†|é…’æ¥¼|çƒ§çƒ¤|é¢é¦†)', '', q_shop).split())
                    doc_keywords = set(re.sub(r'(åº—|é¤å…|é¦†|é…’æ¥¼|çƒ§çƒ¤|é¢é¦†)', '', doc_shop).split())
                    if not q_keywords.intersection(doc_keywords):
                        match = False

            # è¯„åˆ†ç²¾ç¡®åŒ¹é…æˆ–å¤§äºç­‰äº
            if 'rating' in filters:
                try:
                    target = int(filters['rating'])
                    try:
                        doc_rating = int(md.get('rating', '0'))
                    except:
                        doc_rating = 0
                    if doc_rating < target:
                        match = False
                except:
                    # éæ•°å­—åˆ™åšåŒ…å«åŒ¹é…
                    if filters['rating'] != md.get('rating'):
                        match = False

            # å…³æ³¨ç‚¹åŒ¹é…ï¼ˆexactï¼‰
            if 'focus' in filters:
                if filters['focus'] != md.get('focus'):
                    match = False

            # æƒ…æ„Ÿè¿‡æ»¤ï¼ˆpositive/negative/neutralï¼‰
            if 'sentiment' in filters:
                if filters['sentiment'] != md.get('sentiment'):
                    match = False

            if match:
                # æœ€åå†åšä¸€æ¬¡å†…å®¹å…³é”®è¯åŒ¹é…ï¼Œç¡®ä¿ä¸ query è¯­ä¹‰ç›¸å…³ï¼ˆåŸºäºå­ä¸²ï¼‰
                if any(word.lower() in doc['content'].lower() for word in query.split()):
                    filtered_docs.append(doc)

        return filtered_docs


def extract_smart_filters(query: str) -> Dict:
    """ä»ä¸­æ–‡æŸ¥è¯¢ä¸­æŠ½å–è¿‡æ»¤æ¡ä»¶ï¼ˆå¦‚åº—åã€è¯„åˆ†ã€å…³æ³¨ç‚¹ã€æƒ…æ„Ÿï¼‰"""
    filters = {}
    q = query.lower()

    # æå–è¯„åˆ†ï¼šå¦‚ "5æ˜Ÿ"ã€"è¯„åˆ†5"ã€"è‡³å°‘4åˆ†" ç­‰
    m = re.search(r'è‡³å°‘\s*(\d)\s*|(\d)\s*æ˜Ÿ|è¯„åˆ†[:ï¼š]?\s*(\d)', q)
    if m:
        for g in m.groups():
            if g:
                filters['rating'] = g
                break

    # åº—åç®€å•åŒ¹é…ï¼šè‹¥æŸ¥è¯¢ä¸­åŒ…å«æ˜æ˜¾çš„åº—åå…³é”®è¯ï¼ˆç¤ºä¾‹ä¸­åŒ…æ‹¬è€ç‹ã€å°å—ã€ç»¿èŒ¶ã€æµ·é²œï¼‰
    if 'è€ç‹' in q or 'è€ç‹çƒ§çƒ¤' in q:
        filters['shop'] = 'è€ç‹çƒ§çƒ¤'
    elif 'å°å—' in q or 'å°å—é¢é¦†' in q:
        filters['shop'] = 'å°å—é¢é¦†'
    elif 'ç»¿èŒ¶' in q or 'ç»¿èŒ¶é¤å…' in q:
        filters['shop'] = 'ç»¿èŒ¶é¤å…'
    elif 'æµ·é²œ' in q or 'æµ·é²œä¸€å“' in q:
        filters['shop'] = 'æµ·é²œä¸€å“'

    # å…³æ³¨ç‚¹è¯†åˆ«
    if any(k in q for k in ['å£å‘³', 'å‘³é“', 'å¥½åƒ', 'éš¾åƒ']):
        filters['focus'] = 'å£å‘³'
    elif any(k in q for k in ['ç¯å¢ƒ', 'å¹²å‡€', 'é›…è‡´', 'å˜ˆæ‚']):
        filters['focus'] = 'ç¯å¢ƒ'
    elif any(k in q for k in ['æœåŠ¡', 'ä¸Šèœ', 'æ€åº¦']):
        filters['focus'] = 'æœåŠ¡'
    elif any(k in q for k in ['ä»·æ ¼', 'è´µ', 'ä¾¿å®œ', 'äººå‡']):
        filters['focus'] = 'ä»·æ ¼'

    # æƒ…æ„Ÿè¯†åˆ«ï¼ˆå¥½è¯„/å·®è¯„ï¼‰
    if any(p in q for p in ['å¥½è¯„', 'æ¨è', 'æ»¡æ„', 'å–œæ¬¢']):
        filters['sentiment'] = 'positive'
    elif any(n in q for n in ['å·®è¯„', 'å¤±æœ›', 'ä¸æ»¡', 'å·®']):
        filters['sentiment'] = 'negative'

    return filters


def main():
    print("=== ä¸­æ–‡å¤§ä¼—ç‚¹è¯„è¯„è®º RAG æ¼”ç¤º ===")

    # Step 1: åŠ è½½æ–‡æ¡£
    print("ğŸ“š æ­£åœ¨åŠ è½½æ ·æœ¬è¯„è®º...")
    documents = get_sample_documents()

    # Step 2: æŠ½å– metadata
    print("\nğŸ” ä½¿ç”¨å¢å¼ºæŠ½å–ç³»ç»Ÿæå–å…ƒæ•°æ®...")
    extractor = FixedLangExtractProcessor()
    extracted_docs = extractor.extract_metadata(documents)

    # æ˜¾ç¤ºæŠ½å–ç»“æœ
    print("\nğŸ“Š æŠ½å–å¹¶è§„èŒƒåŒ–çš„å…ƒæ•°æ®ï¼š")
    for doc in extracted_docs:
        md = doc['metadata']
        print(f"\n  {doc['id']} ({doc['title']}):")
        print(f"    åº—å: '{md['shop']}'")
        print(f"    è¯„åˆ†: '{md['rating']}'")
        print(f"    å…³æ³¨ç‚¹: '{md['focus']}'")
        print(f"    æƒ…æ„Ÿ: '{md['sentiment']}'")
        if md.get('tags'):
            print(f"    æ ‡ç­¾: {md['tags']}")

    # Step 3: ç´¢å¼•åˆ° SmartVectorStoreï¼ˆå†…å­˜ï¼‰
    print("\nğŸ’¾ ç´¢å¼•æ–‡æ¡£åˆ° SmartVectorStoreï¼ˆæ¨¡æ‹Ÿï¼‰...")
    vector_store = SmartVectorStore()
    vector_store.add_documents(extracted_docs)

    # Step 4: æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "å¦‚ä½•è¯„ä»·è€ç‹çš„å‘³é“ï¼Ÿ",
        "æœ‰å“ªäº› 5æ˜Ÿ çš„æ¨èï¼Ÿ",
        "å…³äºä¸Šèœæ…¢çš„å·®è¯„æœ‰å“ªäº›ï¼Ÿ",
        "ç»¿èŒ¶é¤å… ç¯å¢ƒ æ€ä¹ˆæ ·ï¼Ÿ",
        "æµ·é²œ ä¸€å“ æ˜¯å¦ åè´µï¼Ÿ"
    ]

    print("\nğŸ”¬ æµ‹è¯•æ£€ç´¢ï¼š")
    print("=" * 70)
    for query in test_queries:
        print(f"\nğŸ“ æŸ¥è¯¢: {query}")
        filters = extract_smart_filters(query)
        if filters:
            print(f"   ğŸ¯ æŠ½å–åˆ°çš„è¿‡æ»¤æ¡ä»¶: {filters}")

        with_results = vector_store.search(query, filters)
        print(f"   âœ… ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤æ£€ç´¢åˆ°: {len(with_results)} æ¡è¯„è®º")
        if with_results:
            for r in with_results:
                md = r['metadata']
                print(f"      - {r['id']}: {md['shop']} {md['rating']}æ˜Ÿ ï¼ˆå…³æ³¨ç‚¹: {md['focus']}ï¼Œæƒ…æ„Ÿ: {md['sentiment']})")
        print("\n å®é™…è¿”å›æ–‡æ¡£: ", with_results)

        without_results = vector_store.search(query, None)
        print(f"\n âŒ ä¸ä½¿ç”¨è¿‡æ»¤æ¡ä»¶æ£€ç´¢åˆ°: {len(without_results)} æ¡è¯„è®º")
        print("\n å®é™…è¿”å›æ–‡æ¡£: ", without_results)


if __name__ == "__main__":
    main()

